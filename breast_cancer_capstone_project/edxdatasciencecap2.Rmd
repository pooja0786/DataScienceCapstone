---
title: "**Breast Cancer**"
author: "Pooja Haritwal"
date: "05/02/2022"
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    toc: yes
    toc_depth: 3
    fig_caption: yes
    extra_dependencies: "subfig"
    includes:
      in_header: preamble.tex
  html_document: default
fontsize: 13pt
include-before: '`\newpage{}`{=latex}'
urlcolor: blue
---

```{r setup, include=FALSE}
# Run knitr chunk options
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.align="center", out.width="70%")

# Open required package libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(ggdendro)) install.packages("ggdendro", repos = "http://cran.us.r-project.org")
if(!require(dendextend)) install.packages("dendextend", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(xfun)) install.packages("xfun", repos = "http://cran.us.r-project.org")
if(!require(timeDate)) install.packages("timeDate", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(car)) install.packages("car", repos = "http://cran.us.r-project.org")
if(!require(base)) install.packages("base", repos = "http://cran.us.r-project.org")
if(!require(memisc)) install.packages("memisc", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", dependencies = c("Depends", "Suggests"), repos = "http://cran.us.r-project.org")
library(kableExtra)
# Set all numeric outputs to 3 digits (unless otherwise specified in code chunks)
options(digits = 3)

# Download data-set containing both features and outcome data
data_url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"

# Download data-set description as a 
names_url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names"

# download data and save to data.frame
wdbc_data <- fread(data_url)

# add column names based on information provided in WPBC.names text file
column_names <- c("id", "diagnosis", "radius_m", "texture_m", "perimeter_m", "area_m", "smoothness_m", "compactness_m",
           "concavity_m", "concave_points_m", "symmetry_m", "fractal_dim_m", "radius_se", "texture_se", "perimeter_se", 
           "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se",
           "fractal_dim_se", "radius_w", "texture_w", "perimeter_w", "area_w", "smoothness_w", "compactness_w",
           "concavity_w", "concave_points_w", "symmetry_w", "fractal_dim_w")

colnames(wdbc_data) <- column_names

# Create plot theme to apply to ggplot2 element text throughout report
plot_theme <- theme(plot.caption = element_text(size = 12, face = "italic"), axis.title = element_text(size = 12))
```
\newpage

# **Introduction** 


The Wisconsin breast cancer (diagnostic) database is a set of labelled multivariate data that has been used for classification based machine learning since it was first used in the 1990s and subsequently donated to the UCI machine learning repository. The objective of this project was to use this data set to train different algorithms in order to accurately diagnosis breast cancer based on a prediction as to whether a given sample of cells was malignant or benign tumour mass.

This report sets out the exploratory analysis of the data including the relationship between samples, and features, followed by an overview of the different methods deployed to develop predictive algorithms. The results are presented before a discussion on the relative performance of the models, the limitations of the models as well as the data itself, and opportunities for future work.

\newpage

# **Exploratory Analysis**

## Data set overview

The Breast Cancer Wisconsin (Diagnostic) data set is a `r class(wdbc_data)` consisting of `r ncol(wdbc_data)` columns and `r nrow(wdbc_data)` rows. 

There are `r n_distinct(wdbc_data$id)` unique patient ID numbers, confirming that each row represents a sample from a unique patient. The diagnosis column includes `r class(wdbc_data$diagnosis)` strings that classify whether the samples were diagnosed as benign (B) or malignant (M). The data set consists of `r sum(wdbc_data$diagnosis=="B")` (`r percent(mean(wdbc_data$diagnosis=="B"))`) benign samples and `r sum(wdbc_data$diagnosis=="M")` (`r percent(mean(wdbc_data$diagnosis=="M"))`) malignant samples.


```{r features-list}
# Extract unique features from column names object using stringr functions
features <- column_names[-c(1,2)] %>% str_replace("[^_]+$", "") %>% unique() %>% str_replace_all("_", " ") %>% str_to_title()

# Create data.frame with descriptions for each of the unique features
features_list <- data.frame(Feature = features, Description = 
                              c("Mean of distances from center to points on the perimeter of individual nuclei",
                                "Variance (standard deviation) of grey-scale intensitities in the component pixels",
                                "Perimeter length of each nucleus",
                                "Area as measured by counting pixels within each nucleus",
                                "Local variation in radius lengths",
                                "Combination of perimeter and area using the formula: (perimeter^2 / area - 1.0)",
                                "Number and severity of concavities (indentations) in the nuclear contour",
                                "Number of concavities in the nuclear contour",
                                "Symmetry of the nuclei as measured by length differences between lines perpendicular to the major axis and the cell boundary",
                                "Fractal dimension based on the 'coastline approximation' - 1.0"))

# Format data.frame using kable package
features_list %>%
  kable(caption = "Description of nuclear features", align = 'll', booktabs = T,
        format = "latex", linesep = "") %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("scale_down", "hold_position"))
```

Each of the features described are such that larger values will typically indicate a higher likelihood of malignancy given that they reflect larger cells and/or more irregular shapes. 

```{r feature-mean-summary}
# Create table showing summary data for the mean scores for each of the ten features
summary(wdbc_data[,3:12]) %>%
  kable(caption = "Mean scores", booktabs = T,
        format = "latex", linesep = "") %>%
  kable_styling(position = "center", latex_options = c("scale_down"))
```
```{r feature-worst-summary}
# Create table showing summary data for the worst scores for each of the ten features
summary(wdbc_data[,23:32]) %>%
  kable(caption = "Worst scores", booktabs = T,
        format = "latex", linesep = "") %>%
  kable_styling(position = "center", latex_options = c("scale_down"))
```
```{r feature-se-summary}
# Create table showing summary data for the standard error scores for each of the ten features
summary(wdbc_data[,13:22]) %>%
  kable(caption = "Standard error scores", booktabs = T,
        format = "latex", linesep = "") %>%
  kable_styling(position = "center", latex_options = c("scale_down"))
```

```{r initial-data-tidy}
# remove id column
wdbc_data <- wdbc_data[,-1]

# convert diagnosis to a factor
wdbc_data$diagnosis <- as.factor(wdbc_data$diagnosis)

# Generate features matrix and outcome (diagnosis) list
wdbc <- list(x = as.matrix(wdbc_data[,-1]), y = wdbc_data$diagnosis)
```


The ID column is not required for this project and was removed. The diagnosis column was reclassified as categorical data using the base 'as.factor()' function, with `r nlevels(wdbc_data$diagnosis)` levels, one for benign masses (`r levels(wdbc_data$diagnosis)[1]`) and the other for malignant masses (`r levels(wdbc_data$diagnosis)[2]`).


Prior to normalising the data and exploring distance and clustering of samples and features, the data-set was split into train and test sets in an 80:20 split.

```{r create-train-test-sets}
# Split into train and test sets (80:20)
set.seed(200, sample.kind = "Rounding")
test_index <- createDataPartition(wdbc$y, times = 1, p = 0.2, list = FALSE)
train <- list(x = wdbc$x[-test_index,], y = wdbc$y[-test_index])
test <- list(x = wdbc$x[test_index,], y = wdbc$y[test_index])
```

The balance of classes was consistent between the train set (malignant = `r percent(mean(train$y=="M"), accuracy = 0.1)`) and test set (malignant = `r percent(mean(test$y=="M"), accuracy = 0.1)`). Further data exploration was conducted with the train set only.

```{r centre-scale-train}
# Centre the train$x data around zero by subtracting the column mean 
train_c <- sweep(train$x, 2, colMeans(train$x))

# Scale the train$x data by dividing by the column standard deviation
train_s <- sweep(train_c, 2, colSds(train$x), FUN = "/")
```


```{r sample-distance}
# Use dist function to calculate distance between each sample
d_samples <- dist(train_s)

# Average distance between all samples
ave_dist_samples <- round(mean(as.matrix(d_samples)),2)

# Distance between benign samples
distb_b_samples <- round(mean(as.matrix(d_samples)[train$y=="B"]),2)

# Distance between malignant samples
distm_m_samples <- round(mean(as.matrix(d_samples)[train$y=="M"]),2)

# Distance between benign and malignant samples
distb_m_samples <- round(mean(as.matrix(d_samples)[train$y=="M", train$y=="B"]),2)
```

The train set was normalised using the sweep function firstly to centre each data point ($x$) around zero by subtracting the sample mean ($\overline{x}$) by column and then to scale each data point by dividing by the sample standard deviation ($S$) by column, yielding $z$-scores (1).

\begin{equation}
z = \frac{\left(x - \overline{x}\right)}{S}
\end{equation}

One way of measuring the variance between samples is to calculate the Euclidean distance as

\begin{equation}
d(x_1,x_2) = \sqrt{\sum_{i=1}^{569}\left(x_{1,i}-x_{2,i}\right)^2}
\end{equation}

The average distance between all samples included in the train set was `r ave_dist_samples`. Benign samples were closer to each other (`r distb_b_samples`) than to malignant samples (`r distb_m_samples`). Of note, benign samples were also closer to each other than malignant samples (`r distm_m_samples`) were from each other, indicating greater variance in the measured features in malignant cells.

```{r heatmap-sample-dist, fig.height = 6, out.width="80%", fig.cap="Heatmap of distance between benign (green) and malignant (red) samples"}
# Create heatmap of distance between samples
heatmap(as.matrix(d_samples), symm = T, revC = T,
        col = brewer.pal(4, "Blues"),
        ColSideColors = ifelse(train$y=="B", "green", "red"),
        RowSideColors = ifelse(train$y=="B", "green", "red"),
        labRow = NA, labCol = NA)
```

### Variance within features

```{r feature-variance}
# Identifying zero variance predictors
nzv <- nearZeroVar(train_s, saveMetrics= TRUE)
```

The nearZeroVar function within the caret package demonstrated that none of the features in the train set had either zero or near zero variance. The lowest percentage of unique values for any feature was `r min(nzv$percentUnique)` and the mean of all features was `r mean(nzv$percentUnique)`. Moreover, the mean frequency ratio was `r round(mean(nzv$freqRatio),2)` with `r percent(sum(nzv$freqRatio<=2)/length(nzv$freqRatio))` of all features having a frequency ratio score of 2 or less. This analysis supports the inclusion of all features in the algorithm based on their within feature variance.

### Hierarchical clustering

The train set was transposed so that the features were moved from the columns of the matrix to the rows and the dist function operated to calculate the Euclidean distance between each feature.

```{r feature-clustering, out.width="80%", fig.cap="Dendrogram of hierarchical clusters of features"}
# Hierarchical clustering of features
h <- hclust(dist(t(train_s)))

# Create dendrogram structure from h object
dend <- as.dendrogram(h) %>% set("branches_k_color", k = 7) %>%
  set("labels_cex", 0.5) %>%
  set("labels_colors", k = 7)

# Create ggdend object using dendextend package to visualise with the ggplot2 package
ggd <- as.ggdend(dend)

# Plot dendrogram from ggd object using ggplot2 package
ggplot(ggd, horiz = TRUE, theme = plot_theme) +  labs(x = "Features", y = "Distance")
```

The features that are closest together are those that measure nuclear size, including radius, perimeter and area. Those features that measure shape rather than size are further apart from each other, although concavity, compactness and the number of concave points are relatively near to each other.

### Correlation between features

\begin{equation}
r_{x_1,x_2}=\frac{\sum_{i=1}^{n}\left(x_{1,i}-\overline{x}{_1}\right)\left(x_{2,i}-\overline{x}{_2}\right)}{\sqrt{\sum_{i=1}^{n}\left(x_{1,i}-\overline{x}{_1}\right)^2}\sqrt{\sum_{i=1}^{n}\left(x_{2,i}-\overline{x}{_2}\right)^2}}
\end{equation}

```{r feature-correlation, fig.height = 6, out.width="80%", fig.cap="Heatmap of correlation between features"}
# Identifying correlated predictors
trainCor <- abs(cor(train_s))
cutoff <- 0.9
corr_index <- findCorrelation(trainCor, cutoff = cutoff, names = FALSE)
corr <- findCorrelation(trainCor, cutoff = cutoff, names = TRUE)

# Create heatmap of correlation between features
heatmap(as.matrix(trainCor), col = brewer.pal(9, "RdPu"), labCol = NA, showlegend = NA)
```

Overall, there is a low level of correlation between features; indeed, the mean correlation coefficient of features in the train set is `r round(mean(trainCor),2)`. The greatest correlation is between the various measures of cell size, namely radius, perimeter and area and, to a lesser extent between some of the measures of cell shape, namely fractal dimension, symmetry and smoothness.

`r n2w(length(corr), cap = TRUE)` features have a correlation of `r cutoff` or more, namely `r combine_words(corr)`. Excluding these features from unsupervised methods of developing the predictive algorithm may be beneficial.

### Principal Component Analysis

```{r pca-analysis}
# Save principal component analysis in pca object
pca <- prcomp(train_s)

# Calculate variance scores per principal component
pca.var <- pca$sdev^2
pca.var.per <- pca.var/sum(pca.var)
```

Principal component analysis (PCA) is a technique for transforming data-sets in order to reduce dimensionality without reducing the number of features by identifying the principal components which explain as much of the data variance as possible. PCA can be used to improve visualisation of multidimensional data and, potentially, to improve the predictive accuracy of classification models.

```{r top-ten-pcs}
# Create table showing the first 10 Principal Components
summary(pca)$importance[,1:10] %>%
  kable(caption = "First 10 Principal Components", booktabs = T, format = "latex") %>%
  kable_styling(position = "center", latex_options = c("scale_down", "hold_position"))
```

In most cases the spread is greater for malignant masses than for benign masses. PC1 is the only component for which the interquartile ranges do not overlap. Principal component analysis does not take into account the classification of data, in this case the diagnosis assigned to each sample. 

```{r pca-box-plot, fig.cap="Box plots of top 10 PCs by diagnosis"}
# Create boxplot to show top 10 PCs by diagnosis
data.frame(pca$x[,1:10], Diagnosis = train$y) %>%
    gather(key = "PC", value = "value", -Diagnosis) %>%
    ggplot(aes(PC, value, fill = Diagnosis)) +
    geom_boxplot() +
    scale_fill_discrete(name="Diagnosis",
                        breaks=c("B", "M"),
                        labels=c("Benign", "Malignant"))
```
The graph shows that the malignant data-points are more spread out than the benign data-points and that more of the variance can be accounted for on the $x$-axis (PC1) than on the $y$-axis (PC2). Ellipses help to visualise this even better, firstly with a larger ellipse for malignant data-points than for benign data-points and considerable separation of data by classification, despite some overlap. This analysis support the use of PCA in algorithm development to predict diagnosis from this data-set.

```{r pc1-pc2-plot, fig.cap="Scatter plot of PC1 and PC2 by diagnosis"}
# Create scatterplot of PC1 and PC2 by diagnosis
data.frame(pca$x[,1:2], Diagnosis = train$y) %>%
  ggplot(aes(PC1, PC2, color = Diagnosis)) +
  geom_point() +
  stat_ellipse() +
  xlab(paste("PC1: ", percent(pca.var.per[1],0.1))) +
  ylab(paste("PC2: ", percent(pca.var.per[2],0.1))) +
  scale_color_discrete(name="Diagnosis",
                      breaks=c("B", "M"),
                      labels=c("Benign", "Malignant"))
```

\newpage

# **Methods**

## Pre-processing

The exploratory analysis of the Wisconsin breast cancer (diagnostic) data-set revealed patterns across both samples and features that support the use of machine learning techniques to develop predictive algorithms.


```{r centre-scale-test-set}
## Centre the test$x data around zero by subtracting the train$x column mean 
test_c <- sweep(test$x, 2, colMeans(train$x))

## Scale the test$x data by dividing by the train$x column standard deviation
test_s <- sweep(test_c, 2, colSds(train$x), FUN = "/")
```

An empty data frame was generated in which to store key performance metrics for each model developed, namely the overall accuracy of the model (4), the sensitivity, or true positive rate (5), the specificity, or true negative rate (6).

\begin{equation}
\mbox{Accuracy} = \frac{\mbox{True Positive}+{\mbox{True Negative}}}{\mbox{True Positive}+{\mbox{True Negative}}+{\mbox{False Positive}}+{\mbox{False Negative}}}
\end{equation}

\begin{equation}
\mbox{Sensitivity} = \frac{\mbox{True Positive}}{\mbox{True Positive + False Positive}}
\end{equation}

\begin{equation}
\mbox{Specificity} = \frac{\mbox{True Negative}}{{\mbox{True Negative}}+{\mbox{False Positive}}}
\end{equation}

The F1 score, a harmonic mean of precision, or positive predictive value, and sensitivity (7) is another measure of a model's accuracy and was also included. To aid analysis, the false negative rates (8) and false positive rates (9) were also computed.

\begin{equation}
\mbox{F1 score} = \frac{\mbox{2(True Positive)}}{{\mbox{2(True Positive)}}+{\mbox{False Positive}}+{\mbox{False Negative}}}
\end{equation}

\begin{equation}
\mbox{False Negative Rate} = \frac{\mbox{False Negative}}{\mbox{False Negative + True Positive}} = \mbox{1 - Sensitivity}
\end{equation}

\begin{equation}
\mbox{False Positive Rate} = \frac{\mbox{False Positive}}{\mbox{False Positive + True Negative}} = \mbox{1 - Specificity}
\end{equation}

```{r - test-results-data-frame}
model_results <- data.frame(Method = character(),
                            Accuracy = double(),
                            Sensitivity = double(),
                            Specificity = double(),
                            F1 = double(),
                            FNR = double(),
                            FPR = double())
```

Cross-validation is an important technique used to measure performance of a model without recourse to the test data-set, allowing the test set to be reserved for the final hold-out test of each model and minimising the risk of over-fitting. It is also a useful technique for tuning parameters for those models that require it (e.g., to tune the number of neighbours, $k$, to include in a k-nearest neighbours model).
For the purposes of measuring performance within the resamples only the final predictions and summary performance metrics (accuracy and kappa scores) were saved based on the tuned parameters where applicable. Kappa scores are another measure of the agreement between observed ($p_o$) and expected values ($p_e$) and, unlike overall accuracy, take account of the chance that a prediction (or observed value) will match the true (or expected) value.

\begin{equation}
\kappa = \frac{p_o-p_e}{1-p_e}
\end{equation}

```{r train-control}
# Define train control parameters for appropriate models
fitControl <- trainControl(method = "repeatedcv",
                           number = 10, # 10-fold cross-validation
                           repeats = 10, # repeat each cross-validation 10 times
                           classProbs = TRUE, # class probabilities computed
                           returnResamp = "final", # only save the final resampled summary metrics
                           savePredictions = "final") # only save the final predictions for each resample
```

## Random sampling

The first model to be tested involved random sampling with equal probability for each outcome, i.e. equivalent to a coin toss. Given the imbalance in prevalence of benign and malignant samples in the data-set, a second model was built with weighted random sampling, where the prevalence of each class of samples was used to define the probability of each outcome within the random sample.

```{r random-sample}
# Random sampling model

# Probability for random sampling set to 0.5, i.e. no weighting between different outcomes
p <- 0.5

# Set seed to enable result to be reproduced
set.seed(3, sample.kind = "Rounding")

# Sample outcomes matching length and factor levels from the test set
random_pred <- sample(c("B", "M"), length(test_index), prob = c(1-p, p), replace = TRUE) %>%
  factor(levels = levels(test$y))

# Store confusion matrix in 'random' object
random <- confusionMatrix(random_pred, test$y, positive = "M")

# Add model model results to model_results data frame
model_results[1, ] <- c("Random sample",
                        format(round(random$overall["Accuracy"],2), nsmall = 2),
                        format(round(random$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(random$byClass["Specificity"],2), nsmall = 2),
                        format(round(random$byClass["F1"],2), nsmall = 2),
                        percent(1-(random$byClass["Sensitivity"])),
                        percent(1-(random$byClass["Specificity"])))
```

```{r weighted-random-sample}
# Weighted random sampling model

# Probability for random sampling set to match the prevalence of benign and malignant samples in the train set
p <- mean(train$y=="M")

# Set seed to enable result to be reproduced
set.seed(3, sample.kind = "Rounding")

# Sample outcomes matching length and factor levels from the test set
weighted_random_pred <- sample(c("B", "M"), length(test_index), prob = c(1-p, p), replace = TRUE) %>%
  factor(levels = levels(test$y))

# Store confusion matrix in 'weighted_random' object
weighted_random <- confusionMatrix(weighted_random_pred, test$y, positive = "M")

# Add model results to model_results data frame
model_results[2, ] <- c("Weighted random sample",
                        format(round(weighted_random$overall["Accuracy"],2), nsmall = 2),
                        format(round(weighted_random$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(weighted_random$byClass["Specificity"],2), nsmall = 2),
                        format(round(weighted_random$byClass["F1"],2), nsmall = 2),
                        percent(1-(weighted_random$byClass["Sensitivity"])),
                        percent(1-(weighted_random$byClass["Specificity"])))
```

### k-means clustering

$k$-means clustering is another form of unsupervised modelling where the number of clusters is defined in advance.  $k$-means clustering is an attractive option for large data-sets because it is a relatively simple approach to clustering and as such is computationally faster than hierarchical clustering. 

```{r k-means-predict-function}
# Build k-means prediction function
predict_kmeans <- function(x, k) {
  # extract cluster centres
  centres <- k$centers
  # calculate distance from data-points to the cluster centres
  distances <- sapply(1:nrow(x), function(i){
    apply(centres, 1, function(y) dist(rbind(x[i,], y)))
  })
  # select cluster with min distance to centre
  max.col(-t(distances))
}
```

Two version of the $k$-means model were developed. The first used the normalised data from the full train data-set. The second selected out those features which were highly correlated, i.e. the `r n2w(length(corr))` features where the correlation coefficient exceeded the `r cutoff` cutoff defined in the exploratory analysis. $k$-means clustering places greater weight on larger clusters and variables that are highly correlated (that form a large cluster) may therefore carry greater weight in the prediction algorithm.

```{r k-means-full-data}
# K-means model using full normalised data-set

#Define x as normalised train dataset, train_s
x <- train_s

# Set seed to enable result to be reproduced
set.seed(25, sample.kind = "Rounding")

# Predict outcome using kmeans model with k=2 and 25 random sets
k <- kmeans(x, centers = 2, nstart = 25)
kmeans_pred <- factor(ifelse(predict_kmeans(test_s, k) == 1, "M", "B"))

# Store confusion matrix in 'kmeans_results_1' object
kmeans_results_1 <- confusionMatrix(kmeans_pred, test$y, positive = "M")

# Add model results to model_results data frame
model_results[3, ] <- c("K-means clustering",
                        format(round(kmeans_results_1$overall["Accuracy"],2), nsmall = 2),
                        format(round(kmeans_results_1$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(kmeans_results_1$byClass["Specificity"],2), nsmall = 2),
                        format(round(kmeans_results_1$byClass["F1"],2), nsmall = 2),
                        percent(1-(kmeans_results_1$byClass["Sensitivity"])),
                        percent(1-(kmeans_results_1$byClass["Specificity"])))
```

```{r k-means-feature-selection}
# K-means model with feature selection based on correlation coefficient score

#Define x_select as normalised train data-set, train_s extracting those included in the corr_index based on a correlation coefficient score over the cutoff (0.9)
x_select <- train_s[,-c(corr_index)]

#Apply the same selection process to the test data-set
test_s_select <- test_s[,-c(corr_index)]

# Set seed to enable result to be reproduced
set.seed(25, sample.kind = "Rounding")

# Predict outcome using kmeans model with k=2 and 25 random sets
k <- kmeans(x_select, centers = 2)
kmeans_pred_2 <- factor(ifelse(predict_kmeans(test_s_select, k) == 1, "M", "B"))

# Store confusion matrix in 'kmeans_results_2' object
kmeans_results_2 <- confusionMatrix(kmeans_pred_2, test$y, positive = "M")

# Add model results to model_results data frame
model_results[4, ] <- c("K-means (without highly correlated features)",
                        format(round(kmeans_results_2$overall["Accuracy"],2), nsmall = 2),
                        format(round(kmeans_results_2$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(kmeans_results_2$byClass["Specificity"],2), nsmall = 2),
                        format(round(kmeans_results_2$byClass["F1"],2), nsmall = 2),
                        percent(1-(kmeans_results_2$byClass["Sensitivity"])),
                        percent(1-(kmeans_results_2$byClass["Specificity"])))
```

### Generative modelling


\begin{equation}
p\left(\mathbf{x}\right)=\mbox{Pr}\left(Y=1|\mathbf{X}=\mathbf{x}\right)=\frac{f_{\mathbf{X}|Y=1}\left(\mathbf{x}\right)\mbox{Pr}\left(Y=1\right)}{f_{\mathbf{X}|Y=0}\left(\mathbf{x}\right)\mbox{Pr}\left(Y=0\right)+f_{\mathbf{X}|Y=1}\left(\mathbf{x}\right)\mbox{Pr}\left(Y=1\right)}
\end{equation}

The Naive Bayes model assumes that all features within the data-set are equally important and independent.

Other generative models include linear discriminative analysis (LDA) and quadratic discriminative analysis (QDA).

```{r linear-discriminant-analysis}
# LDA model

# Set seed to enable result to be reproduced
set.seed(30, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using normalised train/test data-sets
train_lda <- train(train_s, train$y, 
                   method = "lda", 
                   trControl = fitControl)
lda_pred <- predict(train_lda, test_s)

# Store confusion matrix in 'lda_results' object
lda_results <- confusionMatrix(lda_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[6, ] <- c("Linear Discriminant Analysis",
                        format(round(lda_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(lda_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(lda_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(lda_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(lda_results$byClass["Sensitivity"])),
                        percent(1-(lda_results$byClass["Specificity"])))
```

```{r quadratic-discriminant-analysis}
# QDA model

# Set seed to enable result to be reproduced
set.seed(32, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using normalised train/test data-sets
train_qda <- train(train_s, train$y, 
                   method = "qda", 
                   trControl = fitControl)
qda_pred <- predict(train_qda, test_s)

# Store confusion matrix in 'qda_results' object
qda_results <- confusionMatrix(qda_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[7, ] <- c("Quadratic Discriminant Analysis",
                        format(round(qda_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(qda_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(qda_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(qda_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(qda_results$byClass["Sensitivity"])),
                        percent(1-(qda_results$byClass["Specificity"])))
```

```{r quadratic-discriminant-analysis-pca}
# QDA model with PCA

# Set seed to enable result to be reproduced
set.seed(32, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using non-normalised train/test data-sets
train_qda_pca <- train(train$x, train$y,
                       method = "qda",
                       trControl = fitControl,
                       # Pre-processing function to centre, scale and apply pca
                       preProcess = c("center", "scale", "pca"))
qda_pca_pred <- predict(train_qda_pca, test$x)

# Store confusion matrix in 'qda_pca_results' object
qda_pca_results <- confusionMatrix(qda_pca_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[8, ] <- c("Quadratic Discriminant Analysis (with PCA)",
                        format(round(qda_pca_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(qda_pca_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(qda_pca_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(qda_pca_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(qda_pca_results$byClass["Sensitivity"])),
                        percent(1-(qda_pca_results$byClass["Specificity"])))
```

#### Logistic Regression

Logistic regression is the most commonly used form of generalised linear model (GLM). Linear regression assumes that the predictor, $X$, and the outcome $Y$, follow a bivariate normal distribution such that the conditional expectation, i.e. the expected outcome $Y$ for a given predictor $X$, fits the regression line.

\begin{equation}
p\left(x\right)=\mbox{Pr}\left(Y=1|X=x\right)=\beta_0+\beta_1x
\end{equation}


\begin{equation}
g\left\{\mbox{Pr}\left(Y=1|X=x\right)\right\}=\beta_0+\beta_1x
\end{equation}

```{r logistic-regression}
# Logistic regression model

# Set seed to enable result to be reproduced
set.seed(36, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using normalised train/test data-sets
train_glm <- train(train_s, train$y, 
                   method = "glm", 
                   trControl = fitControl)
glm_pred <- predict(train_glm, test_s)

# Store confusion matrix in 'glm_results' object
glm_results <- confusionMatrix(glm_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[9, ] <- c("Logistic regression",
                        format(round(glm_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(glm_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(glm_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(glm_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(glm_results$byClass["Sensitivity"])),
                        percent(1-(glm_results$byClass["Specificity"])))
```

```{r logistic-regression-pca}
# Logistic regression model with PCA

# Set seed to enable result to be reproduced
set.seed(36, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using non-normalised train/test data-sets
train_glm_pca <- train(train$x, train$y,
                       method = "glm",
                       trControl = fitControl,
                       # Pre-processing function to centre, scale and apply pca
                       preProcess = c("center", "scale", "pca"))
glm_pca_pred <- predict(train_glm_pca, test$x)

# Store confusion matrix in 'glm_pca_results' object
glm_pca_results <- confusionMatrix(glm_pca_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[10, ] <- c("Logistic regression (with PCA)",
                        format(round(glm_pca_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(glm_pca_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(glm_pca_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(glm_pca_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(glm_pca_results$byClass["Sensitivity"])),
                        percent(1-(glm_pca_results$byClass["Specificity"])))
```

#### Nearest neighbour model

The $k$-Nearest neighbour model ($k$NN) is a simple approach to supervised machine learning that assumes proximity equates to similarity, once again measuring the Euclidean distance between two points in multidimensional data. Unlike hierarchical and $k$-means clustering, the KNN model is a form of supervised learning, i.e. it relies on and makes use of the diagnosis labels in the training set in order to predict diagnosis in an unlabelled test set.

```{r k-nearest-neighbour}
# KNN model

# Set seed to enable result to be reproduced
set.seed(5, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using normalised train/test data-sets
train_knn <- train(train_s, train$y,
                   method = "knn",
                   tuneGrid = data.frame(k = seq(1, 30, 2)),
                   trControl = fitControl)
knn_pred <- predict(train_knn, test_s)

# Store confusion matrix in 'knn_results' object
knn_results <- confusionMatrix(knn_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[11, ] <- c("K Nearest Neighbour",
                        format(round(knn_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(knn_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(knn_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(knn_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(knn_results$byClass["Sensitivity"])),
                        percent(1-(knn_results$byClass["Specificity"])))
```

#### Random forest model

Many algorithms, including some of those described above, suffer from diminished performance due to multidimensionality of data. As has been described, PCA can be useful to reduce the number of dimensions required as part of pre-processing of data prior to training one of these algorithms. Decisions trees are another way to address this issue, effectively partitioning the data such that final predictions can be made on a smaller subset of predictors.

```{r random-forest-rf}
# Random Forest model

# Set seed to enable result to be reproduced
set.seed(7, sample.kind = "Rounding")

# Use caret package to train and then predict outcomes using normalised train/test data-sets
train_rf <- train(train_s, train$y,
                  method = "rf",
                  tuneGrid = data.frame(mtry = seq(3, 15, 2)),
                  importance = TRUE,
                  trControl = fitControl)
rf_pred <- predict(train_rf, test_s)

# Store confusion matrix in 'rf_results' object
rf_results <- confusionMatrix(rf_pred, test$y, positive = "M")

# Add results to model_results data frame
model_results[12, ] <- c("Random Forest",
                         format(round(rf_results$overall["Accuracy"],2), nsmall = 2),
                         format(round(rf_results$byClass["Sensitivity"],2), nsmall = 2),
                         format(round(rf_results$byClass["Specificity"],2), nsmall = 2),
                         format(round(rf_results$byClass["F1"],2), nsmall = 2),
                         percent(1-(rf_results$byClass["Sensitivity"])),
                         percent(1-(rf_results$byClass["Specificity"])))
```

### Ensemble model

Ensembles are combinations of individual model predictions that seek to improve both stability and accuracy of the final result, just as the random forest algorithm uses combinations of individual decision trees. There is no established convention for selecting which models to include in the ensemble. One approach is to establish a performance cutoff within the training sets, via cross-validation, in order to avoid selection based on performance in the test set.

```{r ensemble-model}
# Build ensemble model with all supervised models tested
ensemble <- cbind(glm = ifelse(glm_pred == "B", 0, 1), glm_pca = ifelse(glm_pca_pred == "B", 0, 1), lda = ifelse(lda_pred == "B", 0, 1), qda = ifelse(qda_pred == "B", 0, 1), qda_pca = ifelse(qda_pca_pred == "B", 0, 1), rf = ifelse(rf_pred == "B", 0, 1), knn = ifelse(knn_pred == "B", 0, 1))

# Predict final classification based on majority vote across all models, using the rowMeans function
ensemble_preds <- as.factor(ifelse(rowMeans(ensemble) < 0.5, "B", "M"))

# Store confusion matrix in 'ensemble_results' object
ensemble_results <- confusionMatrix(ensemble_preds, test$y, positive = "M")

# Add results to model_results data frame
model_results[14, ] <- c("Ensemble",
                        format(round(ensemble_results$overall["Accuracy"],2), nsmall = 2),
                        format(round(ensemble_results$byClass["Sensitivity"],2), nsmall = 2),
                        format(round(ensemble_results$byClass["Specificity"],2), nsmall = 2),
                        format(round(ensemble_results$byClass["F1"],2), nsmall = 2),
                        percent(1-(ensemble_results$byClass["Sensitivity"])),
                        percent(1-(ensemble_results$byClass["Specificity"])))
```

\newpage

# **Results**

## Overall performance

```{r results-table}
# Print table of results from each model using kable
model_results %>%
  kable(caption = "Key performance metrics for each model", 
        align = 'lrrrrrr',
        booktabs = T,
        format = "latex",
        linesep = "") %>%
  kable_styling(full_width = FALSE,
                position = "center", 
                latex_options = c("scale_down", "hold_position")) %>%
  pack_rows(index = c("Random sampling" = 2,
                      "Unsupervised models" = 2,
                      "Generative models" = 4,
                      "Discriminative models" = 5,
                      "Ensemble" = 1)) %>%
  footnote(general = "FNR = false negative rate; FPR = false positive rate; PCA = principal component analysis")
```

$k$-means clustering improved accuracy, with an F1 score of `r model_results[3,5]` but still not to an acceptable level, with a false negative rate of `r model_results[3,6]` and a false positive rate of `r model_results[3,7]`. Of note, removing the highly correlated features (i.e. those with a correlation coefficient above `r cutoff`) from the data-set reduced the accuracy of the $k$-means clustering model, yielding a reduced F1 score of `r model_results[4,5]` and reducing both the specificity and, in particular, the sensitivity of the model.

Linear discriminant analysis and quadratic discriminant analysis improved on the performance of the Naive Bayes model, with F1 scores of `r model_results[6,5]` and `r model_results[7,5]` respectively. The LDA model achieved a specificity of `r model_results[6,4]` (i.e. FPR of `r model_results[6,7]`) but this was offset by reduced sensitivity (`r model_results[6,3]`), i.e. an unacceptable FNR of `r model_results[6,6]`.

The QDA model delivered a better balance between FNR (`r model_results[7,6]`) and FPR (`r model_results[7,7]`). Of note, dimension reduction through pre-processing the training data with PCA improved the specificity of the QDA model, reducing the FPR to `r model_results[8,7]` but it reduced the sensitivity, i.e. increased the FNR to `r model_results[8,6]`.

The discriminative models of supervised learning were the best performing models with this data-set. Logistic regression achieved an overall accuracy of `r model_results[9,2]`. This was improved further to `r model_results[10,2]` with dimension reduction using PCA by improving the sensitivity of the model, achieving FNR and FPR of only `r model_results[10,6]` and `r model_results[10,7]` respectively.

The nearest neighbours model performed best when the number of neighbours, $k$, was defined as `r n2w(train_knn$bestTune[1,1])`. On this basis, the overall accuracy of `r model_results[11,2]` but with lower sensitivity (`r model_results[11,3]`).

```{r tuning-knn-model, fig.cap = "Tuning results for nearest neighbour model during cross-validation"}
# Plot accuracy during cross-validation for each value of k (number of neighbours) tuned
plot(train_knn,
     ylab = "Accuracy (repeated cross-validation)",
     xlab = "# Neighbours (k)")
```

## Variable importance

```{r variable-importance, fig.ncol = 2, out.width = "50%", fig.align = "center", fig.cap = "Variable importance", fig.subcap=c("K Nearest Neighbours (metric: ROC curve)", "Random Forest (metric: model specific)", "Neural Network (metric: model specific)", "Logistic Regression (metric: model specific)")}

# Create varImp objects for each of the nearest neighbour, random forest, neural network and logistic regression models
varimp_knn <- (varImp(train_knn))
varimp_rf <- (varImp(train_rf))
varimp_glm <- (varImp(train_glm))

# Define the number of variables to include in plots ranked by importance
top <- 10

# Generate variable importance plots for each of the varImp objects
plot(varimp_knn, top = top)
plot(varimp_rf, top = top) 
plot(varimp_glm, top = top)
```

The worst performing discriminative model, logistic regression appears to be most reliant on mean and standard error scores for features related to nuclear shape in the top five variables of importance, namely, `r combine_words(varimp_glm$importance %>% top_n(5) %>% rownames())`.

Looking at the best performing models, by comparison, the worst scores feature heavily in the top five variables of importance for the nearest neighbour model (`r varimp_knn$importance %>% top_n(5) %>% rownames() %>% str_detect(pattern = "_w") %>% sum()` out of 5), the random forest model (`r varimp_rf$importance %>% top_n(5) %>% rownames() %>% str_detect(pattern = "_w") %>% sum()` out of 5)

# **Discussion**

Despite the success of the neural network, there is merit in selecting the ensemble of supervised models as the preferred algorithm given that it also correctly predicted diagnosis and has the benefit of mitigating the risk of over-training with an individual model, making it more likely to provide reproducible results in different data-sets that include the same feature information.

The current dataset suffers from a number of inherent biases that represent possible limitations to the reproducibilty of the performance achieved here. The samples were collected from a single site and from a consecutive series of patients. Operator biases will have included those responsible for conducting the biopsies, digitising the images to measure each of the features and even the clinical diagnoses made to classify each sample as benign or malignant. The methods for capturing nuclear size and shape information in 1995 were relatively rudimentary and more advanced image processing techniques available today would complement the complex machine learning algorithms (such as convolutional neural networking) now available to capture differences between benign and malignant samples.

# **Conclusions**

The objective of this project was to use the Wisconsin breast cancer data set to train different algorithms to accurately diagnosis breast cancer. An exploratory analysis of the data revealed that measures of both distance and correlation of nuclear features could be useful in both clustering and classifying individual samples. All of the models developed performed better than random sampling but supervised learning was more accurate than unsupervised learning, and discriminative models were more effective than generative models. The neural network was the most successful individual model, with perfect accuracy within the test data and this performance was matched with an ensemble of supervised models.

These results confirm the potential of machine learning to accurately predict diagnosis of breast cancer using samples obtained via fine needle aspiration biopsy, with high levels of both sensitivity and specificity.